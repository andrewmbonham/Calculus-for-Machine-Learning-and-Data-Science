# Calculus-for-Machine-Learning-and-Data-Science

This course is part of the [Mathematics for Machine Learning and Data Science Specialization](https://www.deeplearning.ai/courses/mathematics-for-machine-learning-and-data-science-specialization/) by [DeepLearning.AI](https://www.deeplearning.ai).

After completing this course, learners will be able to:
- Analytically optimize different types of functions commonly used in machine learning using properties of derivatives and gradients
- Approximately optimize different types of functions commonly used in machine learning using first-order (gradient descent) and second-order (Newton’s method) iterative methods
- Visually interpret differentiation of different types of functions commonly used in machine learning
- Perform gradient descent in neural networks with different activation and cost functions 

### Week 1: Functions of one variable: Derivative and optimization

#### Lesson 1: Derivatives
- Example to motivate derivatives: Speedometer
- Derivative of common functions (c, x, x^2, 1/x)
- Meaning of e and the derivative of e^x
- Derivative of log x
- Existence of derivatives
- Properties of derivative
- Practice Quiz: Derivatives
- Lab: Differentiation in Python: Symbolic, Numerical and Automatic

#### Lesson 2: Optimization with derivatives
- Intro to optimization: Temperature example
- Optimizing cost functions in ML: Squared loss
- Optimizing cost functions in ML: Log loss
- Quiz: Derivatives and Optimization
- Programming Assignment: Optimizing Functions of One Variable: Cost Minimization

### Week 2: Functions of two or more variables: Gradients and gradient descent

#### Lesson 1: Gradients and optimization
- Intro to gradients
- Example to motivate gradients: Temperature
- Gradient notation
- Optimization using slope method: Linear regression
- Practice Quiz: Partial Derivatives and Gradient

#### Lesson 2: Gradient Descent
- Optimization using gradient descent: 1 variable
- Optimization using gradient descent: 2 variable
- Gradient descent for linear regression
- Lab: Optimization Using Gradient Descent in One Variable
- Lab: Optimization Using Gradient Descent in Two Variables
- Quiz: Partial Derivatives and Gradient Descent
- Programming Assignment: Optimization Using Gradient Descent: Linear Regression

### Week 3: Optimization in Neural Networks and Newton’s method

#### Lesson 1: Optimization in Neural Networks
- Perceptron with no activation and squared loss (linear regression)
- Perceptron with sigmoid activation and log loss (classification)
- Two-layer neural network with sigmoid activation and log loss
- Mathematics of Backpropagation
- Lab: Regression with Perceptron
- Lab: Classification with Perceptron
- Practice Quiz: Optimization in Neural Networks

#### Lesson 2: Beyond Gradient Descent: Newton’s Method
- Root finding with Newton’s method
- Adapting Newton’s method for optimization
- Second derivatives and Hessians
- Multivariate Newton’s method
- Lab: Optimization Using Newton's Method
- Quiz: Optimization in Neural Networks and Newton's Method
- Programming Assignment: Neural Network with Two Layers
